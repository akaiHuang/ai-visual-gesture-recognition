<p align="center">
  <img src="https://img.shields.io/badge/Python-3.10+-3776AB?style=for-the-badge&logo=python&logoColor=white" alt="Python" />
  <img src="https://img.shields.io/badge/PyQt6-6.5+-41CD52?style=for-the-badge&logo=qt&logoColor=white" alt="PyQt6" />
  <img src="https://img.shields.io/badge/MediaPipe-0.10+-00A6D6?style=for-the-badge&logo=google&logoColor=white" alt="MediaPipe" />
  <img src="https://img.shields.io/badge/OpenCV-4.8+-5C3EE8?style=for-the-badge&logo=opencv&logoColor=white" alt="OpenCV" />
  <img src="https://img.shields.io/badge/Apple_Metal-GPU_Accelerated-000000?style=for-the-badge&logo=apple&logoColor=white" alt="Metal" />
</p>

# AI Visual Gesture Recognition

### Real-time Hand Tracking and Gesture Classification System

> A desktop application that performs **real-time hand detection, landmark tracking, and gesture classification** using computer vision and AI, with native **macOS Metal GPU acceleration**.
>
> AI é©…å‹•çš„å³æ™‚æ‰‹å‹¢è¾¨è­˜ç³»çµ± -- çµåˆ MediaPipe æ‰‹éƒ¨è¿½è¹¤èˆ‡è‡ªè¨‚æ‰‹å‹¢åˆ†é¡æ¨¡å‹ï¼Œæ”¯æ´ macOS Metal ç¡¬é«”åŠ é€Ÿã€‚

## About

AI Visual Gesture Recognition æ˜¯ä¸€å¥—å³æ™‚æ‰‹éƒ¨è¿½è¹¤èˆ‡æ‰‹å‹¢åˆ†é¡ç³»çµ±ï¼Œå°‡æ”å½±æ©Ÿç•«é¢è½‰ç‚ºå¯ç”¨æ–¼äº’å‹•çš„æ‰‹å‹¢äº‹ä»¶ã€‚é©åˆç”¨æ–¼ HCI åŸå‹ã€äº’å‹•è£ç½®æ§åˆ¶èˆ‡å±•ç¤ºå‹æ‡‰ç”¨ï¼Œå¿«é€Ÿé©—è­‰ä»¥è¦–è¦ºæ‰‹å‹¢å–ä»£æ»‘é¼ /éµç›¤è¼¸å…¥çš„æ“ä½œæµç¨‹ã€‚

## ğŸ“‹ Quick Summary

> âœ‹ **AI Visual Gesture Recognition** æ˜¯ä¸€å¥—åŸºæ–¼é›»è…¦è¦–è¦ºçš„å³æ™‚æ‰‹å‹¢è¾¨è­˜æ¡Œé¢æ‡‰ç”¨ç¨‹å¼ã€‚ç³»çµ±æ¡ç”¨ ğŸ” Google MediaPipe é€²è¡Œ 21 é»æ‰‹éƒ¨é—œéµé»è¿½è¹¤ï¼Œæ”¯æ´é›™æ‰‹åŒæ™‚åµæ¸¬ï¼Œæ­é…è‡ªç ”çš„ ğŸ§  è¦å‰‡å¼æ‰‹å‹¢åˆ†é¡å¼•æ“ï¼Œå¯ç²¾æº–è¾¨è­˜ 8 ç¨®æ‰‹å‹¢é¡å‹ï¼šæ¡æ‹³ âœŠã€å¼µé–‹æ‰‹æŒ ğŸ–ï¸ã€æ¯”è®š ğŸ‘ã€YA âœŒï¸ã€OK ğŸ‘Œã€æŒ‡å‘ ğŸ‘†ã€æ–æ»¾ ğŸ¤Ÿã€ä¸‰æŒ‡ ğŸ¤ï¼Œæ¯å€‹æ‰‹å‹¢é™„å¸¶ 0-1 ä¿¡å¿ƒåº¦åˆ†æ•¸ã€‚æ¡Œé¢ä»‹é¢ä»¥ ğŸ–¥ï¸ PyQt6 æ‰“é€ ï¼Œæ”¯æ´ macOS åŸç”Ÿ Apple Metal GPU ç¡¬é«”åŠ é€Ÿï¼Œå¤§å¹…æå‡æ¸²æŸ“èˆ‡æ¨è«–æ•ˆèƒ½ã€‚å…§å»º ğŸ“Š å³æ™‚æ•ˆèƒ½ç›£æ§å„€è¡¨æ¿ï¼Œå¯è¿½è¹¤ CPUã€è¨˜æ†¶é«”ã€GPU ä½¿ç”¨ç‡ä¸¦è¨­å®šè­¦å‘Šé–¾å€¼ã€‚æ¨¡å‹æ¶æ§‹æ¡ç”¨ ğŸ”Œ æŠ½è±¡åŸºé¡è¨­è¨ˆçš„å¯æ’æ‹”ä»‹é¢ï¼Œç›®å‰ä½¿ç”¨åŸºæ–¼æ‰‹æŒ‡å½æ›²è§’åº¦çš„è¦å‰‡å¼•æ“ï¼Œæœªä¾†å¯ç„¡ç¸«æ›¿æ›ç‚º LSTMã€Transformer ç­‰æ·±åº¦å­¸ç¿’æ¨¡å‹ã€‚æ ¸å¿ƒæŠ€è¡“æ£§ç‚º ğŸ Python + OpenCV + MediaPipe + PyQt6ï¼Œé©åˆé›»è…¦è¦–è¦ºç ”ç©¶èˆ‡äººæ©Ÿäº’å‹•åŸå‹é–‹ç™¼ ğŸš€ã€‚

---

## â­ Highlights / å°ˆæ¡ˆäº®é»

| Feature | Description |
|---------|-------------|
| **Real-time Hand Tracking** | MediaPipe é©…å‹•çš„ 21 é»æ‰‹éƒ¨é—œéµé»è¿½è¹¤ï¼Œæ”¯æ´é›™æ‰‹åŒæ™‚åµæ¸¬ |
| **8 Gesture Types** | è¾¨è­˜æ¡æ‹³ã€å¼µé–‹æ‰‹æŒã€æ¯”è®šã€YAã€OKã€æŒ‡å‘ã€æ–æ»¾ã€ä¸‰æŒ‡ç­‰æ‰‹å‹¢ |
| **Confidence Scoring** | æ¯å€‹æ‰‹å‹¢é™„å¸¶ä¿¡å¿ƒåº¦åˆ†æ•¸ (0-1)ï¼ŒåŸºæ–¼æ‰‹æŒ‡å½æ›²è§’åº¦ç²¾ç¢ºè¨ˆç®— |
| **Metal GPU Acceleration** | macOS åŸç”Ÿ Metal å¾Œç«¯åŠ é€Ÿ PyQt6 æ¸²æŸ“èˆ‡ MediaPipe æ¨è«– |
| **Performance Dashboard** | å³æ™‚ç›£æ§ CPUã€è¨˜æ†¶é«”ã€GPU ä½¿ç”¨ç‡ï¼Œå«è­¦å‘Šé–¾å€¼ç³»çµ± |
| **Extensible Model API** | æŠ½è±¡åŸºé¡è¨­è¨ˆï¼Œæ”¯æ´ç„¡ç¸«æ›¿æ›ç‚º LSTM / Transformer ç­‰æ·±åº¦å­¸ç¿’æ¨¡å‹ |

---

## ğŸ—ï¸ Architecture / ç³»çµ±æ¶æ§‹

```
ai-visual-gesture-recognition/
â”‚
â”œâ”€â”€ main.py                          # Desktop App (737 lines)
â”‚                                    # PyQt6 ä¸»è¦–çª—ã€æ”å½±æ©Ÿä¸²æµã€
â”‚                                    # UI ä½ˆå±€ã€å³æ™‚é è¦½èˆ‡æ‰‹å‹¢é¡¯ç¤º
â”‚
â”œâ”€â”€ config.py                        # Configuration
â”‚                                    # Metal åŠ é€Ÿã€æ”å½±æ©Ÿåƒæ•¸ã€
â”‚                                    # MediaPipe è¨­å®šã€æ•ˆèƒ½é–¾å€¼
â”‚
â”œâ”€â”€ models/
â”‚   â””â”€â”€ gesture_model.py             # Gesture AI Model (239 lines)
â”‚                                    # GestureModel æŠ½è±¡åŸºé¡
â”‚                                    # DummyModel - è¦å‰‡å¼ 8 æ‰‹å‹¢è¾¨è­˜
â”‚                                    # MLModel - æ·±åº¦å­¸ç¿’æ¨¡å‹æ¥å£ï¼ˆé ç•™ï¼‰
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ hand_detector.py             # Hand Detection (130 lines)
â”‚   â”‚                                # MediaPipe Hands å°è£
â”‚   â”‚                                # 21 é—œéµé»æå–ã€å·¦å³æ‰‹è¾¨è­˜
â”‚   â”‚
â”‚   â””â”€â”€ performance_monitor.py       # Performance Monitor (318 lines)
â”‚                                    # CPU/Memory/GPU å³æ™‚ç›£æ§
â”‚                                    # macOS Metal GPU å°ˆç”¨ç›£æ§
â”‚
â”œâ”€â”€ benchmark.py                     # Benchmark Suite
â”‚                                    # æ•ˆèƒ½è©•ä¼°èˆ‡å£“åŠ›æ¸¬è©¦
â”‚
â”œâ”€â”€ screenshot/                      # Demo Screenshots
â”‚   â””â”€â”€ (5 screenshots)
â”‚
â””â”€â”€ requirements.txt                 # Dependencies
```

---

## ğŸ› ï¸ Tech Stack / æŠ€è¡“æ£§

```
  Vision        MediaPipe Hands  |  21-point landmark tracking
  Detection     OpenCV 4.8+  |  Real-time camera feed processing
  UI Framework  PyQt6 6.5+  |  Native desktop application
  GPU Accel.    macOS Metal backend  |  QSG_RHI_BACKEND=metal
  Monitoring    psutil + subprocess  |  CPU / Memory / Metal GPU
  Model API     Abstract Base Class  ->  Rule-based / ML pluggable
```

---

## âœ‹ Gesture Recognition / æ‰‹å‹¢è¾¨è­˜

### ğŸ¤š Supported Gestures / æ”¯æ´çš„æ‰‹å‹¢é¡å‹

| Gesture | Chinese | Detection Method | Confidence |
|---------|---------|-----------------|------------|
| **Fist** | æ¡æ‹³ | All fingers curled | 0.95 |
| **Open Palm** | å¼µé–‹æ‰‹æŒ | All fingers extended | 0.95 |
| **Thumbs Up** | æ¯”è®š | Only thumb extended | 0.95 |
| **Peace / YA** | æ¯” YA | Index + middle extended | 0.90 |
| **OK** | æ¯” OK | Thumb-index circle + 3 fingers up | 0.85 |
| **Pointing** | æŒ‡å‘ | Only index extended | 0.90 |
| **Rock** | æ–æ»¾ | Pinky extended, others curled | 0.85 |
| **Three** | ä¸‰ | Three fingers extended | 0.80 |

### âš™ï¸ Recognition Pipeline / è¾¨è­˜æµç¨‹

```
Camera Frame                         Gesture Output
    |                                     ^
    v                                     |
 [BGR -> RGB]                     [Gesture + Confidence]
    |                                     ^
    v                                     |
 [MediaPipe Hands]               [Rule Engine / ML Model]
    |                                     ^
    v                                     |
 [21 Landmarks (x,y,z)]  --->  [Finger Extension Detection]
                                [Joint Angle Calculation]
                                [Thumb-Index Distance]
```

---

## ğŸ Quick Start / å¿«é€Ÿé–‹å§‹

### ğŸ“¦ 1. Install Dependencies / å®‰è£ä¾è³´

```bash
# Create virtual environment (recommended)
python -m venv venv
source venv/bin/activate

# Install packages
pip install -r requirements.txt
```

**Dependencies / ä¾è³´å¥—ä»¶:**

| Package | Version | Purpose |
|---------|---------|---------|
| `opencv-python` | >= 4.8.0 | Camera capture & image processing |
| `numpy` | >= 1.24.0 | Numerical computation |
| `mediapipe` | >= 0.10.0 | Hand landmark detection |
| `PyQt6` | >= 6.5.0 | Desktop UI framework |
| `psutil` | >= 5.9.0 | System performance monitoring |
| `gputil` | >= 1.4.0 | GPU monitoring (optional) |

### ğŸš€ 2. Run the Application / å•Ÿå‹•æ‡‰ç”¨

```bash
python main.py
```

Or use the provided script:

```bash
chmod +x run.sh
./run.sh
```

### ğŸ’¡ 3. Usage / ä½¿ç”¨æ–¹å¼

```
1. Select a camera source from the dropdown
   å¾ä¸‹æ‹‰é¸å–®é¸æ“‡æ”å½±æ©Ÿ

2. Click "Start Detection" to begin hand tracking
   é»æ“Šã€Œé–‹å§‹åµæ¸¬ã€å•Ÿå‹•æ‰‹éƒ¨è¿½è¹¤

3. Hold your hand in front of the camera
   å°‡æ‰‹æ”¾åœ¨æ”å½±æ©Ÿå‰æ–¹

4. Recognized gestures appear in real-time with confidence scores
   è¾¨è­˜çµæœèˆ‡ä¿¡å¿ƒåº¦å³æ™‚é¡¯ç¤º
```

---

## ğŸ“ˆ Performance / æ•ˆèƒ½è¡¨ç¾

### ğŸ Metal GPU Acceleration (macOS)

The system automatically enables Apple Metal acceleration on macOS:

```python
# Enabled in config.py
os.environ["QSG_RHI_BACKEND"] = "metal"       # PyQt6 Metal rendering
os.environ["MEDIAPIPE_DISABLE_GPU"] = "0"      # MediaPipe GPU inference
```

### âš ï¸ Performance Thresholds / æ•ˆèƒ½è­¦å‘Šé–¾å€¼

| Metric | Warning | Danger |
|--------|---------|--------|
| CPU Usage | 70% | 100% |
| Memory | 500 MB | 1000 MB |
| GPU Usage | 70% | 90% |

### ğŸ§ª Benchmarking / æ•ˆèƒ½æ¸¬è©¦

```bash
python benchmark.py
```

---

## ğŸ§  Model Architecture / æ¨¡å‹æ¶æ§‹

The project uses a **pluggable model interface** via abstract base class:

```python
class GestureModel(ABC):
    """Abstract base class for gesture recognition models"""

    @abstractmethod
    def predict(self, landmarks: np.ndarray) -> Dict[str, Any]:
        """Input: (21, 3) landmarks -> Output: {gesture, confidence, details}"""
        pass

# Current: Rule-based recognition using finger angles
class DummyModel(GestureModel): ...

# Future: Drop-in ML model replacement
class MLModel(GestureModel): ...    # LSTM / Transformer / CNN
```

### ğŸ”¢ Finger Detection Algorithm / æ‰‹æŒ‡åµæ¸¬æ¼”ç®—æ³•

```
For each finger (index, middle, ring, pinky):
    extended = tip.y < pip.y    (fingertip above second joint)

For thumb:
    extended = |tip.x - mcp.x| > 0.04    (horizontal displacement)

Joint angles calculated via:
    angle = arccos( dot(BA, BC) / (|BA| * |BC|) )
```

---

## âš™ï¸ Configuration / è¨­å®šåƒæ•¸

All configurable via `config.py`:

| Parameter | Default | Description |
|-----------|---------|-------------|
| `CAMERA_WIDTH` | 640 | Camera resolution width |
| `CAMERA_HEIGHT` | 480 | Camera resolution height |
| `CAMERA_FPS` | 30 | Target frame rate |
| `MEDIAPIPE_MAX_HANDS` | 2 | Max simultaneous hands |
| `MEDIAPIPE_MIN_DETECTION_CONFIDENCE` | 0.7 | Detection threshold |
| `MEDIAPIPE_MIN_TRACKING_CONFIDENCE` | 0.5 | Tracking threshold |
| `MEDIAPIPE_MODEL_COMPLEXITY` | 0 | 0=lite (fast), 1=full (accurate) |
| `UI_UPDATE_INTERVAL_MS` | 33 | UI refresh rate (~30 FPS) |

---

## ğŸ“Š Project Stats / å°ˆæ¡ˆçµ±è¨ˆ

```
Total Files:     35
Language:        Python + PyQt6
Main App:        737 lines (main.py)
Model Layer:     239 lines (gesture_model.py)
Hand Detector:   130 lines (hand_detector.py)
Perf. Monitor:   318 lines (performance_monitor.py)
```

---

## ğŸ“„ License

MIT License

---

<p align="center">
  <sub>Built with MediaPipe, PyQt6, and OpenCV for real-time computer vision research.</sub>
  <br />
  <sub>çµåˆ MediaPipeã€PyQt6 èˆ‡ OpenCV æ‰“é€ çš„å³æ™‚é›»è…¦è¦–è¦ºç ”ç©¶æ‡‰ç”¨ã€‚</sub>
</p>
